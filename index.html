<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
  <!-- Replace the content tag with appropriate information -->
  <meta name="description" content="DESCRIPTION META TAG">
  <meta property="og:title" content="SOCIAL MEDIA TITLE TAG"/>
  <meta property="og:description" content="SOCIAL MEDIA DESCRIPTION TAG TAG"/>
  <meta property="og:url" content="URL OF THE WEBSITE"/>
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X630-->
  <meta property="og:image" content="static/image/your_banner_image.png" />
  <meta property="og:image:width" content="1200"/>
  <meta property="og:image:height" content="630"/>


  <meta name="twitter:title" content="TWITTER BANNER TITLE META TAG">
  <meta name="twitter:description" content="TWITTER BANNER DESCRIPTION META TAG">
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
  <meta name="twitter:image" content="static/images/your_twitter_banner_image.png">
  <meta name="twitter:card" content="summary_large_image">
  <!-- Keywords for your paper to be indexed by-->
  <meta name="keywords" content="KEYWORDS SHOULD BE PLACED HERE">
  <meta name="viewport" content="width=device-width, initial-scale=1">


  <title>ICGAN: AN IMPLICIT CONDITIONING METHOD FOR INTERPRETABLE FEATURE CONTROL
    OF NEURAL AUDIO SYNTHESIS</title>
  <link rel="icon" type="image/x-icon" href="static/images/favicon.ico">
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
  rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/bulma/0.9.3/css/bulma.min.css">
  <link rel="stylesheet"
  href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
</head>
<body>


  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">ICGAN</h1>
            <h2 class="">AN IMPLICIT CONDITIONING METHOD FOR INTERPRETABLE FEATURE CONTROL
              OF NEURAL AUDIO SYNTHESIS</h2>
            <div class="is-size-5 publication-authors">
              <!-- Paper authors -->
              <span class="author-block">
                <a href="FIRST AUTHOR PERSONAL LINK" target="_blank">Yunyi Liu</a>,</span>
                <span class="author-block">
                  <a href="SECOND AUTHOR PERSONAL LINK" target="_blank">Craig Jin</a>,</span>
                  </div>

                  <div class="is-size-5 publication-authors">
                    <span class="author-block">University of Sydney<br>Electrical and Information Engineering</span>

                  <div class="column has-text-centered">
                    <div class="publication-links">
                         <!-- Arxiv PDF link -->
                      <span class="link-block">
                        <a href="https://arxiv.org/pdf/<ARXIV PAPER ID>.pdf" target="_blank"
                        class="external-link button is-normal is-rounded is-dark">
                        <span class="icon">
                          <i class="fas fa-file-pdf"></i>
                        </span>
                        <span>Paper</span>
                      </a>
                    </span>

                  <!-- Github link -->
                  <span class="link-block">
                    <a href="https://github.com/Reinliu/ICGAN" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                  </a>
                </span>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>


<!-- Teaser image-->
<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <img src="static/ICGAN/Architecture.png" id="tree" alt="Architecture Image" style="width: 100%;">
      <h2 class="subtitle has-text-centered">
        We propose an implicit conditioning method to achieve smooth interpolation between different classes of 
        sounds with discrete labels. As shown in our architecture, our generator is conditioned on a vector sampled 
        from a Gaussian distribution with mean and variance parameterized from the encoder. The encoder learns
        the categorical information from the input Mel spectrograms and guides the generator in an implicit way.
      </h2>
    </div>
  </div>
</section>
<!-- End teaser image -->

<!-- Paper abstract -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Neural audio synthesis methods can achieve high-fidelity and realistic sound generation by utilizing 
            deep generative models. However, such models typically fail to provide convenient or interpretable 
            controls to guide the sound generation, especially with regard to sounds that are hard to describe by 
            words. This paper proposes an implicit conditioning method for neural audio synthesis that allows for 
            interpretable control of the acoustic features of synthesized sounds.  Our technique creates a 
            continuous conditioning space that enables timbre manipulation without relying on explicit labels. 
            We further introduce an evaluation metric to explore controllability and demonstrate that our approach 
            is effective in enabling a degree of controlled variation of different sound effects for same-class 
            and cross-class sounds.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End paper abstract -->


<!-- Interpolation -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Interpolation</h2>
        <div class="content has-text-justified">
          <p>
            We interpolate our conditioning space between two classes of sounds, one starting a value at 0 while
            another starting at 1. We show the output probabilities returned from the 
          </p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End paper interpolation -->


<!-- In-class sounds -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Generated sounds with implicit conditioning</h2>
        <div class="content has-text-justified">
          <p>
            During the inference stage, we could generate Mel spectrograms by inputting self-defined sampled
            vectors. As the learned sampled vectors are expected to have a mean of 0 when the associated class
            feature is absent and a mean of 1 when the associated class feature is present, we manually set the 
            sampled vectors with 1 at a class dimension and 0 for everywhere else. Below we show the generated 
            Mel spectrograms and transformed sounds conditioned on self-defined vectors.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End in-class sounds -->

<!-- Column 3 -->
<div class="column">
  <h4 class="title is-4">DDSP-SFX 1</h4>
  <audio preload='metadata' controls>
      <source src='Reconstruction/Gunshots/DDSP-VAE_037.wav' type='audio/wav'>
  </audio>
  <figure class="image">
      <img src="Reconstruction/Gunshots/ddsp-vae-037.png" alt="DDSP-SFX 1">
  </figure>

  <h4 class="title is-4">DDSP-SFX 2</h4>
  <audio preload='metadata' controls>
      <source src='Reconstruction/Gunshots/DDSP-VAE_048.wav' type='audio/wav'>
  </audio>
  <figure class="image">
      <img src="Reconstruction/Gunshots/ddsp-vae-048.png" alt="DDSP-SFX 2">
  </figure>

  <!-- Add three more sets below -->
  <h4 class="title is-4">DDSP-SFX 3</h4>
  <audio preload='metadata' controls>
      <source src='Reconstruction/Gunshots/DDSP-VAE_049.wav' type='audio/wav'>
  </audio>
  <figure class="image">
      <img src="Reconstruction/Gunshots/ddsp-vae-049.png" alt="DDSP-SFX 3">
  </figure>

  <h4 class="title is-4">DDSP-SFX 4</h4>
  <audio preload='metadata' controls>
      <source src='Reconstruction/Gunshots/DDSP-VAE_050.wav' type='audio/wav'>
  </audio>
  <figure class="image">
      <img src="Reconstruction/Gunshots/ddsp-vae-050.png" alt="DDSP-SFX 4">
  </figure>

  <h4 class="title is-4">DDSP-SFX 5</h4>
  <audio preload='metadata' controls>
      <source src='Reconstruction/Gunshots/DDSP-VAE_051.wav' type='audio/wav'>
  </audio>
  <figure class="image">
      <img src="Reconstruction/Gunshots/ddsp-vae-051.png" alt="DDSP-SFX 5">
  </figure>
</div>



<!-- Out-of-class sounds -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Generated sounds with implicit conditioning</h2>
        <div class="content has-text-justified">
          <p>
            During the inference stage, we could generate Mel spectrograms by inputting self-defined sampled
            vectors. As the learned sampled vectors are expected to have a mean of 0 when the associated class
            feature is absent and a mean of 1 when the associated class feature is present, we manually set the 
            sampled vectors with 1 at a class dimension and 0 for everywhere else. 
          </p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End out-of-class sounds -->

<!--BibTex citation -->
  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
      <pre><code>BibTex Code Here</code></pre>
    </div>
</section>
<!--End BibTex citation -->


  <footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">

          <p>
            This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" 
            target="_blank">Academic Project Page Template</a> which was adopted from the 
            <a href="https://nerfies.github.io" target="_blank">Nerfies</a> project page.
            You are free to borrow the of this website, we just ask that you link back to this page in the footer. 
            <br> This website is licensed under a <a rel="license"  href="http://creativecommons.org/licenses/by-sa/4.0/" 
            target="_blank">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>

        </div>
      </div>
    </div>
  </div>
</footer>

<!-- Statcounter tracking code -->
  
<!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

    <!-- End of Statcounter Code -->

  </body>
  </html>
